{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb48b101-e30b-498f-8f50-73e99bf2a92e",
   "metadata": {},
   "source": [
    "# Bias and Explainability\n",
    "\n",
    "In this notebook, we will explore the concepts of bias and explainability in machine learning. We will use [the Adult Census Income public dataset](https://archive.ics.uci.edu/dataset/2/adult) (also referred to as the \"Adult\" or \"Adult income\" dataset), which is known to contain imbalances with regarding to gender and race.\n",
    "\n",
    "Dataset Citation: Becker,Barry and Kohavi,Ronny. (1996). Adult. UCI Machine Learning Repository. https://doi.org/10.24432/C5XW20."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f54b89-2bb7-4bb2-a69e-dddbe055e03e",
   "metadata": {},
   "source": [
    "## Basic setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec1c534-09a1-49e1-a6a5-3fd1b0417081",
   "metadata": {},
   "source": [
    "### Set Google Cloud resource variables\n",
    "\n",
    "The following code will set variables specific to your Google Cloud resources that will be used in this notebook, such as the Project ID, Region, and GCS Bucket.\n",
    "\n",
    "**Note: This notebook is intended to execute in a Vertex AI Workbench Notebook, in which case the API calls issued in this notebook are authenticated according to the permissions (e.g., service account) assigned to the Vertex AI Workbench Notebook.**\n",
    "\n",
    "We will use the `gcloud` command to get the Project ID details from the local Google Cloud project, and assign the results to the PROJECT_ID variable. If, for any reason, PROJECT_ID is not set, you can set it manually or change it, if preferred.\n",
    "\n",
    "We also use a default bucket name for most of the examples and activities in this book, which has the format: `{PROJECT_ID}-aiml-sa-bucket`. You can change the bucket name if preferred.\n",
    "\n",
    "Also, we're defaulting to the **us-central1** region, but you can optionally replace this with your [preferred region](https://cloud.google.com/about/locations)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae6de65-79e8-46ba-a1e8-2959d22f96dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID_DETAILS = !gcloud config get-value project\n",
    "PROJECT_ID = PROJECT_ID_DETAILS[0]  # The project ID is item 0 in the list returned by the gcloud command\n",
    "BUCKET=f\"{PROJECT_ID}-aiml-sa-bucket\" # Optional: replace with your preferred bucket name, which must be a unique name.\n",
    "REGION=\"us-central1\" # Optional: replace with your preferred region (See: https://cloud.google.com/about/locations) \n",
    "print(f\"Project ID: {PROJECT_ID}\")\n",
    "print(f\"Bucket Name: {BUCKET}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "868271aa-3ec1-45d7-be9d-c36aef4ef395",
   "metadata": {},
   "source": [
    "### Create bucket\n",
    "\n",
    "The following code will create the bucket if it doesn't already exist.\n",
    "\n",
    "If you get an error saying that it already exists, that's fine, you can ignore it and continue with the rest of the steps, unless you want to use a different bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c2de5e-c2ef-40a1-a49e-9614f1107b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil mb -l us-central1 gs://{BUCKET}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1592827-4f0c-442c-beb7-04e18f2bb58e",
   "metadata": {},
   "source": [
    "## Begin implementation\n",
    "\n",
    "Now that we have performed the prerequisite steps for this activity, it's time to implement the activity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa8cec8-a149-465c-8c54-12381aac94d1",
   "metadata": {},
   "source": [
    "## Bias\n",
    "\n",
    "We start first by exploring the contents of the dataset, to explore any potential biases that may be inherently present in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8329888e-ee12-4490-98f0-51bba145a01b",
   "metadata": {},
   "source": [
    "### Import libraries and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7089d015-4ed3-4cee-aca2-9d8e9fa56974",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Define column names since the dataset doesn't have a header row\n",
    "column_names = ['age', 'workclass', 'fnlwgt', 'education', 'education_num', 'marital_status', 'occupation',\n",
    "                'relationship', 'race', 'sex', 'capital_gain', 'capital_loss', 'hours_per_week', 'native_country', 'income']\n",
    "\n",
    "# Load the dataset\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data\"\n",
    "adult_data = pd.read_csv(url, names=column_names, sep=r'\\s*,\\s*', engine='python')\n",
    "\n",
    "adult_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfd30896-c8fc-43bf-b673-c21d4cbaa1a4",
   "metadata": {},
   "source": [
    "### Start with a summary of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9d73230-d53a-4e41-9918-09f257f12fc1",
   "metadata": {},
   "source": [
    "Get an overview of the dataset, such as the column names, the data type in each column, and the number of non-null rows for each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af4fb453-3bf5-4f13-a335-d743a35adf8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "adult_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8829fbe3-2486-47f1-9e9b-86d3ed1710a3",
   "metadata": {},
   "source": [
    "View summary statistics for the data, such as the count, mean, standard deviation, min, max, and percentiles for each numeric feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd3598e-bfea-4482-a5a4-413dd02e414d",
   "metadata": {},
   "outputs": [],
   "source": [
    "adult_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5caf5cf-4cbc-49f7-91bd-807f1e853002",
   "metadata": {},
   "source": [
    "### View distributions by gender and race\n",
    "\n",
    "Explore whether income appears to be equally or unequally distributed by gender and race."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "767ab25c-939b-4bd9-a75f-c5c9ec675864",
   "metadata": {},
   "source": [
    "#### Income Distribution by Gender "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a088482b-abda-4034-a8d0-6dad83e96951",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "matplotlib.rcParams.update({'font.size': 15})\n",
    "sns.countplot(x='income', hue='sex', data=adult_data)\n",
    "plt.title('Income Distribution by Gender')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf192ef-c572-4d68-a493-55bb351b07d7",
   "metadata": {},
   "source": [
    "In the resulting graph, we can see that, overall, more people in the dataset earn more than $50,000 per year. We can also see that the numbers or males in each group far exceed the number of females in each group. This also tells us the entire dataset consists of more datapoints related to men than women. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e22ff67c-0fea-4daa-84e9-72b96abde816",
   "metadata": {},
   "source": [
    "#### Income Distribution by Race:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1238a5bf-1c5c-4f63-842b-74954b73b590",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 8))\n",
    "sns.countplot(x='income', hue='race', data=adult_data)\n",
    "plt.title('Income Distribution by Race')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "872e8866-5438-4c74-ad57-213005182ab1",
   "metadata": {},
   "source": [
    "Some important things to note from the graph outputs:\n",
    "\n",
    "In both the \"<=50K\" category, and the \">50K\" category, we see that the data contains many more data points for White people than any other race. This can be seen as a type of bias in the dataset. This bias may exist for multiple potential reasons, such as bias in the collection of the data, or it may happen due to other factors such as geographic location. For example, this particular dataset represents the population of a specific area, which may somewhat explain its apparent bias towards a particular race. For example, if the data were collected in Asia then it would contain many more data points for Asian people than any other race, or if it were collected in central Africa then it would contain many more data points for Black people than any other race. It's important to note any imbalances in the data and determine how they may affect the training of a machine learning model. \n",
    "\n",
    "Generally, if features in the dataset have much higher numbers of instances of a specific value, then an ML model's predictions will likely reflect that in some way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dac6083-48b3-4ac4-ab0f-46c249f5533e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of occupation by gender\n",
    "plt.figure(figsize=(20, 8))\n",
    "sns.countplot(y='occupation', hue='sex', data=adult_data)\n",
    "plt.title('Occupational Distribution by Gender')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e05cd5-f9de-4e08-a081-7ca54c9ce8fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of education by race\n",
    "plt.figure(figsize=(15, 8))\n",
    "sns.countplot(y='education', hue='race', data=adult_data, order=adult_data['education'].value_counts().index)\n",
    "plt.title('Educational Distribution by Race')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f49a6bd4-5868-4eec-8dbb-f3a010d56bcf",
   "metadata": {},
   "source": [
    "## Disparate Impact\n",
    "\n",
    "The code in the following cell first creates a pivot table of the adult_data DataFrame, grouped by gender and income, with the count of people in each group as the value. Then, it adds a new column to the pivot table called rate, which is the proportion of people in each group who earn more than $50,000. Finally, it calculates the Disparate Impact (DI) by dividing the rate for females by the rate for males."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "691872e0-3aea-4bed-8084-fc73ae4def44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, we'll focus on the gender-based disparity in income:\n",
    "\n",
    "# Create a pivot table of the income outcome across genders\n",
    "pivot_gender_income = adult_data.pivot_table(index='sex', columns='income', values='age', aggfunc='count')\n",
    "\n",
    "# Calculate the favorable outcome rate for each gender\n",
    "# Here, the favorable outcome is earning >50K\n",
    "pivot_gender_income['rate'] = pivot_gender_income['>50K'] / (pivot_gender_income['>50K'] + pivot_gender_income['<=50K'])\n",
    "\n",
    "# Calculate Disparate Impact (DI) as the ratio of rates between the genders\n",
    "# Using 'Female' as the protected group and 'Male' as the reference group\n",
    "DI = pivot_gender_income.loc['Female', 'rate'] / pivot_gender_income.loc['Male', 'rate']\n",
    "\n",
    "print(f\"Disparate Impact (Female vs Male): {DI:.3f}\")\n",
    "\n",
    "# If DI is not between 0.8 and 1.25, it might be indicative of potential bias.\n",
    "if DI < 0.8 or DI > 1.25:\n",
    "    print(\"Potential gender-based bias detected in income.\")\n",
    "else:\n",
    "    print(\"No gender-based bias detected in income.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c2c53c-921d-4bc9-b50c-ad78ee7aa39f",
   "metadata": {},
   "source": [
    "# Explainability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f22e3acb-e158-435a-9474-d24f4c37840f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.inspection import PartialDependenceDisplay\n",
    "\n",
    "# Load the dataset\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data\"\n",
    "column_names = ['age', 'workclass', 'fnlwgt', 'education', 'education-num', 'marital-status', 'occupation', \n",
    "                'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', 'hours-per-week', 'native-country', 'income']\n",
    "data = pd.read_csv(url, names=column_names, sep='\\s*,\\s*', engine='python')\n",
    "\n",
    "# Basic preprocessing\n",
    "data['income'] = data['income'].apply(lambda x: 1 if x == \">50K\" else 0)\n",
    "data = pd.get_dummies(data, drop_first=True)\n",
    "\n",
    "# Drop 'fnlwgt' as it is a compound feature and not relevant for the examples in this notebook\n",
    "data = data.drop('fnlwgt', axis=1)\n",
    "\n",
    "# Separate the target from the input features\n",
    "X = data.drop('income', axis=1)\n",
    "y = data['income']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a86d08b8-621b-494a-83c4-19370f867f7b",
   "metadata": {},
   "source": [
    "## Feature importance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a287b49d-23bd-4915-adb5-49dfdf5e1c56",
   "metadata": {},
   "source": [
    "The following code looks simple, but it's achieving quite a lot. Let's break it down, line by line:\n",
    "\n",
    "**feat_importances = pd.Series(clf.feature_importances_, index=X.columns)**\n",
    "\n",
    "* clf.feature_importances_: This is an attribute of certain scikit-learn estimators, particularly tree-based estimators like Decision Trees, Random Forests, and Gradient Boosted Trees. It gives an array of importance scores for each feature. Higher values indicate more importance.\n",
    "\n",
    "* X.columns: This retrieves the column names of the X DataFrame. This will be used to label each of the importance scores with the appropriate feature name.\n",
    "\n",
    "* pd.Series(...): This constructs a pandas Series, which is a one-dimensional labeled array, using the feature importance scores as values and the column names of X as the index (labels).\n",
    "\n",
    "Overall, this line creates a pandas Series where each item is labeled with a feature name and has a value corresponding to the importance of that feature as determined by the model.\n",
    "\n",
    "**feat_importances.nlargest(10).plot(kind='barh')**\n",
    "\n",
    "* feat_importances.nlargest(10): This selects the top 10 largest feature importance scores from the feat_importances Series. The resulting Series will only have 10 items, each representing one of the top 10 most important features.\n",
    "\n",
    "* .plot(kind='barh'): This is a pandas method to plot the Series. The kind='barh' argument specifies that it should be a horizontal bar plot. Each bar represents a feature, and the length of the bar indicates the feature's importance.\n",
    "\n",
    "To summarize all of the above, this code retrieves the feature importance scores from the model, constructs a labeled pandas Series with those scores, selects the top 10 most important features, plots their importance as a horizontal bar chart, and then displays the plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ff3445-f1b2-4ad5-812f-2b08c6e6ef2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_importances = pd.Series(clf.feature_importances_, index=X.columns)\n",
    "feat_importances.nlargest(10).plot(kind='barh')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7315f5a-099d-4afa-97cf-4840dc2e30eb",
   "metadata": {},
   "source": [
    "### Partial Dependence Plots (PDPs)\n",
    "\n",
    "The code in the next cell will perform the following steps:\n",
    "\n",
    "1. Specify two feature names, 'age' and 'hours-per-week'. These are the features for which Partial Dependence Plots will be generated.\n",
    "2. Generating and Displaying PDPs:\n",
    "\n",
    "PartialDependenceDisplay.from_estimator(): \n",
    "This is a static method provided by the PartialDependenceDisplay class in the sklearn.inspection module. It is used to compute and display the partial dependence plots for the specified features.\n",
    "\n",
    "Parameters passed to from_estimator():\n",
    "* clf: This is the trained RandomForest classifier machine learning model. The method will use this model to compute the average predictions over the grid values of the features.\n",
    "\n",
    "* X: This is the dataset on which the partial dependence is computed. The data is used to determine the unique values or grid points of the features for which the PDPs are to be plotted.\n",
    "\n",
    "* features: The list of feature names for which PDPs are to be generated. \n",
    "\n",
    "Since two features are passed, the method will generate:\n",
    "\n",
    " - A PDP for the 'age' feature: This will show how the model's predictions change, on average, as the 'age' feature varies while other features are kept constant.\n",
    " - A PDP for the 'hours-per-week' feature: Similarly, this will show how predictions vary with changes in the 'hours-per-week' feature.\n",
    " - A 2D PDP showing interactions between 'age' and 'hours-per-week': This plot will reveal how the model's predictions change with different combinations of values for both 'age' and 'hours-per-week'.\n",
    "\n",
    "The method will compute the partial dependence for the given features and then display the PDPs. The plots provide a visual representation of how the specified features impact the model's predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "705ffb45-a72a-4afc-b142-c21b6f073edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['age', 'hours-per-week']\n",
    "PartialDependenceDisplay.from_estimator(clf, X, features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "623aeb48-f95b-4e52-a030-2887012b57e1",
   "metadata": {},
   "source": [
    "## LIME\n",
    "\n",
    "In the following cells, I will provide a brief example for using LIME to explain individual predictions from a classifier. The original paper describing LIME can be found [here](https://arxiv.org/abs/1602.04938).\n",
    "\n",
    "The code in the following cells will perform these steps:\n",
    "\n",
    "1. Install LIME.\n",
    "2. Import the LimeTabularExplainer class from the lime package, which is specifically designed for explaining predictions from models that operate on tabular data.\n",
    "3. Create an Explainer Object, which is an instance of LimeTabularExplainer. We initialize it with the following parameters:\n",
    " - X_train.values: The training data as a NumPy array which the model has been trained on.\n",
    " - training_labels=y_train: The labels associated with the training data.\n",
    " - feature_names=X.columns.tolist(): The names of the features in the dataset, which help in making the explanation human-readable.\n",
    " - class_names=['<=50K', '>50K']: The names of the classes that the model is predicting. In this case, it's a binary classification problem with two possible outcomes: whether someone makes less than or equal to $50K, or more than $50K annually.\n",
    " - mode='classification': This indicates that the explainer is being used for a classification problem (as opposed to a regression problem).\n",
    "4. Select a random instance for explanation (i = np.random.randint(0, X_test.shape[0])). This instance will be used to generate an explanation for the model's prediction.\n",
    "5. Generate the explanation for the selected instance from the test set. The method clf.predict_proba is passed to the explainer, which is a function that the classifier clf uses to predict probabilities for the input data. The parameter num_features=10 tells the explainer to only use the top 10 features that are most influential for this particular prediction.\n",
    "6. Displaying the explanation: The visualization will show the features that influenced the model's prediction for the selected test instance, along with their relative importance and contribution to the prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef224fee-22dd-4c10-a9f2-db2cd832f354",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --quiet lime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f659a38-9fc3-482c-9167-ac87f998e119",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lime import lime_tabular\n",
    "\n",
    "explainer = lime_tabular.LimeTabularExplainer(X_train.values, training_labels=y_train, feature_names=X.columns.tolist(), \n",
    "                                              class_names=['<=50K', '>50K'], mode='classification')\n",
    "\n",
    "i = np.random.randint(0, X_test.shape[0])\n",
    "exp = explainer.explain_instance(X_test.values[i], clf.predict_proba, num_features=10)\n",
    "exp.show_in_notebook()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5790a735-ff80-4362-9119-11418cc80288",
   "metadata": {},
   "source": [
    "## Counterfactuals\n",
    "\n",
    "Next, we will use counterfactuals to understand what minimal changes would affect the model's decision-making process. The code will generate counterfactual explanations for a subset of instances from our test dataset. More specifically, for the first 100 instances in our test dataset, it will perform these steps:\n",
    "\n",
    "1. Predict the original class.\n",
    "2. Make a copy of the instance and modifies two features:\n",
    "* adds 5 years to the feature at index 0 (age).\n",
    "* subtracts 10 hours from the feature at index 12 ('hours-per-week').\n",
    "3. Check for change in prediction: the classifier clf is used to predict the class of the perturbed instance. If the prediction is different from the original (i.e., equals the target_class), the perturbed instance is used as a counterfactual example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed853191-3d44-4366-ad66-beecfe769f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from sklearn.exceptions import DataConversionWarning\n",
    "\n",
    "warnings.filterwarnings(action='ignore', category=UserWarning)\n",
    "\n",
    "counterfactuals = []\n",
    "\n",
    "#for i in range(len(X_test)):\n",
    "for i in range(100):\n",
    "    instance = X_test.iloc[i].values\n",
    "    \n",
    "    # Flip the predicted class\n",
    "    target_class = 1 - clf.predict([instance])\n",
    "    \n",
    "    # Perturb the instance\n",
    "    perturbed_instance = instance.copy()\n",
    "    perturbed_instance[0] += 5  # adding 5 years to age\n",
    "    perturbed_instance[12] -= 10  # decreasing 10 hours from hours-per-week\n",
    "    \n",
    "    if clf.predict([perturbed_instance]) == target_class:\n",
    "        counterfactuals.append((i, \"Modified age and hours-per-week to flip the prediction\"))\n",
    "\n",
    "if counterfactuals:\n",
    "    for index, msg in counterfactuals:\n",
    "        print(f\"For instance {index}: {msg}\")\n",
    "else:\n",
    "    print(\"No counterfactuals found by modifying age and hours-per-week for any instance.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67bfc8cd-7326-417f-8701-14903ce56c7d",
   "metadata": {},
   "source": [
    "## SHAP\n",
    "\n",
    "For in-depth information about how the sampled Shapley method works, read the paper [Bounding the Estimation Error of Sampling-based Shapley Value Approximation](https://arxiv.org/abs/1306.4265)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f0234f-a472-43f7-b1d1-7fe536a3a908",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --quiet shap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a849d89b-4fdc-49de-820f-0072d723b07e",
   "metadata": {},
   "source": [
    "The code in the next cell will perform the following steps:\n",
    "\n",
    "**shap_values = shap.TreeExplainer(clf).shap_values(X_test)**\n",
    "\n",
    "* shap.TreeExplainer(clf): Here, a [TreeExplainer](https://shap.readthedocs.io/en/latest/generated/shap.TreeExplainer.html) object from the SHAP library is being created. This specific explainer is optimized for tree-based models, such as Decision Trees, Random Forests, and Gradient Boosted Trees. The clf that we pass to it is the tree-based model that we've trained earlier in this notebook.\n",
    "\n",
    "* .shap_values(X_test): Once the explainer is created, we use the .shap_values() method to compute the SHAP values for each sample in X_test.\n",
    "\n",
    "SHAP values quantify the contribution of each feature to the model's prediction for a particular instance, relative to the average prediction for the entire dataset.\n",
    "\n",
    "For a binary classification problem, the method will return a list with two arrays: one for each class. The first array is for the negative class (class 0) and the second array is for the positive class (class 1). Each array will have a shape (number of instances, number of features).\n",
    "\n",
    "**shap.summary_plot(shap_values, X_test, plot_type=\"bar\")** \n",
    "\n",
    "* shap.summary_plot(...): This function from the SHAP library creates a summary plot, combining feature importance with feature effects. Each point on the summary plot is a Shapley value for a feature and an instance. The position on the y-axis is determined by the feature and on the x-axis by the Shapley value. This means the plot provides a view of the model’s output in terms of feature importance and feature impact.\n",
    "\n",
    "* shap_values: The SHAP values.\n",
    "\n",
    "* X_test: This is the data for which the SHAP values are computed. It's passed to the function to map the SHAP values back to their corresponding features, making the plot interpretable.\n",
    "\n",
    "* plot_type=\"bar\": This argument specifies the type of summary plot. In this case, a bar plot is generated. Each bar represents a feature, and the length of the bar shows the feature's average impact on the model output. The features are sorted by their average absolute SHAP value over all the samples in X_test.\n",
    "\n",
    "Overall, we're computing the SHAP values for the positive class on the X_test data, and then visualizing the average impact of each feature on the model's predictions using a bar plot. The longer the bar, the greater the feature's importance.\n",
    "\n",
    "**Note: The following code can take an hour or more to complete.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "358d84c2-4cd6-4a2e-93fd-b5f3494bb836",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "\n",
    "shap_values = shap.TreeExplainer(clf).shap_values(X_test)\n",
    "shap.summary_plot(shap_values, X_test, plot_type=\"bar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac238be8-dad3-4353-b6e4-c96798d934aa",
   "metadata": {},
   "source": [
    "Let's discuss why clf.feature_importances_ and SHAP might give different rankings for feature importance:\n",
    "\n",
    "Computation:\n",
    "\n",
    "Feature Importances (from clf.feature_importances_): For tree-based models, the importance of a feature is computed as the (normalized) total reduction of the criterion (like Gini impurity or mean squared error) brought by that feature. It's a cumulative metric across all the trees in the model. Features that tend to split closer to the root of trees in the ensemble typically receive higher importance.\n",
    "\n",
    "SHAP values: SHAP values are based on cooperative game theory. The SHAP value for a feature is the average contribution of that feature value to every possible prediction (averaged over all instances). It takes into account intricate interactions with other features, as well as the feature's main effect.\n",
    "\n",
    "Interactions:\n",
    "\n",
    "Feature Importances: This method often overlooks feature interactions. If one feature entirely captures the information of another feature, the latter might appear as unimportant even if it's crucial in the presence of other features.\n",
    "\n",
    "SHAP values: SHAP considers both main effects and interaction effects. It provides a more detailed view of how each feature and its interactions contribute to predictions.\n",
    "\n",
    "Bias:\n",
    "\n",
    "Feature Importances: The method has known biases. For instance, it might favor features with more categories or more split points. In a decision tree, high-cardinality features can lead to more splits and thus might appear artificially important.\n",
    "\n",
    "SHAP values: SHAP values attempt to be consistent, meaning if we change the model such that it relies more on a feature, the attributed importance of that feature should not decrease. It provides a more balanced view that is less subject to biases inherent in the training process.\n",
    "\n",
    "Global vs. Local:\n",
    "\n",
    "Feature Importances: Provides a global perspective of feature importance averaged over the entire dataset.\n",
    "\n",
    "SHAP values: While individual SHAP values are local (instance-specific), the summary plot gives a global perspective by aggregating over all instances. This allows capturing more complex patterns and relationships in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a1da977-bbc9-4fa6-811f-f6c8043ac012",
   "metadata": {},
   "source": [
    "### Get explanations from the deployed model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d3f9160-7a48-4f1f-a14d-bf758cf303b4",
   "metadata": {},
   "source": [
    "Conveniently, Vertex AI provides APIs and an SDK that we can use to get explanations from our models. In this section, we will use the `projects.locations.endpoints.explain` API to get explanations from the model that we deployed in our MLOps pipeline in the previous chapter. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b91a21e-34a8-4721-a727-a0e641541283",
   "metadata": {},
   "source": [
    "#### Import and initialize the Vertex AI SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab7b472-7264-4a51-86de-f4dbebb273bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import aiplatform\n",
    "aiplatform.init(project=PROJECT_ID, location=REGION)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c3a9f9e-22d2-4069-993c-8f4a353e0ab7",
   "metadata": {},
   "source": [
    "#### Get the test data we created in the MLOps chapter to test our model and get explanations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdcf60d4-1de3-4c14-816f-d17bbf3cc867",
   "metadata": {},
   "source": [
    "First, set up constants:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2bf9c6e-9769-4b4e-b9f0-88d7a5c109b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_DATA_PREFIX = \"test_data\" \n",
    "TEST_DATA_DIR = f\"{TEST_DATA_PREFIX}_dir\"\n",
    "TEST_DATA_FILE_NAME = f\"{TEST_DATA_PREFIX}.jsonl\"\n",
    "TEST_DATASET_PATH = f\"{BUCKET_URI}/{TEST_DATA_FILE_NAME}\"\n",
    "LOCAL_TEST_DATASET_PATH = f\"./{TEST_DATA_DIR}/{TEST_DATA_FILE_NAME}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1bf1962-04b2-47d8-92a1-4b5a4c31160f",
   "metadata": {},
   "source": [
    "Create a local directory to store the test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d0e1b7-4013-420e-950c-089f5029c810",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p $TEST_DATA_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b54e6da0-9dd9-41e6-a905-5f2717772f85",
   "metadata": {},
   "source": [
    "Copy the test data to our local directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a525ec21-830b-436e-af8e-4592763f9edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "! gsutil cp $TEST_DATASET_PATH $TEST_DATA_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a799cb85-a7a4-4368-84b0-f3553cb7da3d",
   "metadata": {},
   "source": [
    "#### Specify endpoint details\n",
    "\n",
    "This is the Vertex AI endpoint on which our model is hosted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0186c9bc-4b3f-4461-8026-729aa2a55b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "ENDPOINT_NAME = \"mlops-endpoint\"\n",
    "mlops_endpoint_list = aiplatform.Endpoint.list(filter=f'display_name={ENDPOINT_NAME}', order_by='create_time desc')\n",
    "new_mlops_endpoint = mlops_endpoint_list[0]\n",
    "endpoint_resource_name = new_mlops_endpoint.resource_name\n",
    "print(endpoint_resource_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf18c46-6f49-4ae1-809a-587ff027c920",
   "metadata": {},
   "source": [
    "#### Get explanations from the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef54dedd-868f-47e0-a788-dc56e9edcda6",
   "metadata": {},
   "source": [
    "We will call the endpoint to get explanations for a datapoint from our test data, which we have stored in a local file. \n",
    "\n",
    "In this case, we're using the Sampled Shapely method which assigns credit for the outcome to each feature. This method provides a sampling approximation of exact Shapely values. Further information on the attribution methods for explanations can be found at [Overview of Explainable AI](https://cloud.google.com/vertex-ai/docs/explainable-ai/overview)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e37c052-5796-40c4-9cd1-22307ac1eb26",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "\n",
    "def explain_tabular_sample(\n",
    "    project: str, location: str, endpoint_name: str, instance_dict: Dict\n",
    "):\n",
    "    endpoint = aiplatform.Endpoint(endpoint_name)\n",
    "\n",
    "    response = endpoint.explain(instances=[instance_dict], parameters={})\n",
    "    \n",
    "    for explanation in response.explanations:\n",
    "        print(\" explanation\")\n",
    "        # Feature attributions.\n",
    "        attributions = explanation.attributions\n",
    "        for attribution in attributions:\n",
    "            print(\"  attribution\")\n",
    "            print(\"   baseline_output_value:\", attribution.baseline_output_value)\n",
    "            print(\"   instance_output_value:\", attribution.instance_output_value)\n",
    "            # Convert feature_attributions to a dictionary and print\n",
    "            feature_attributions_dict = dict(attribution.feature_attributions)\n",
    "            print(\"   feature_attributions:\", feature_attributions_dict)\n",
    "            print(\"   approximation_error:\", attribution.approximation_error)\n",
    "            print(\"   output_name:\", attribution.output_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d122e0a8-6a8c-4c0b-b307-3f25d4649cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(LOCAL_TEST_DATASET_PATH, 'r') as f:\n",
    "    # Read the first line of the file\n",
    "    line = f.readline()\n",
    "\n",
    "    # Convert JSON line to Python dictionary\n",
    "    instance = json.loads(line)\n",
    "    \n",
    "    # Convert to a list of lists (required for our model input)\n",
    "    instance_list = [instance]\n",
    "\n",
    "    # Send the inference request\n",
    "    explain_tabular_sample(project=PROJECT_ID, location=REGION, endpoint_name=endpoint_resource_name, instance_dict=instance_list[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf9974b7-93a7-43b2-a4c1-1ba22fc92e42",
   "metadata": {},
   "source": [
    "#### Understanding the response\n",
    "\n",
    "Let's break down each element of the response:\n",
    "\n",
    "**explanation:** This indicates the start of the explanation for a given instance.\n",
    "\n",
    "**attribution:** Within each explanation, there are feature attributions. These attributions assign a value to each feature in our instance to explain how much each feature influenced the model's prediction.\n",
    "\n",
    "**baseline_output_value:** This is the model's output value for the baseline instance. A baseline is a reference point (like an average or neutral instance) against which the prediction for your instance of interest is compared. In many explanation methods, the difference in the model's output between the instance of interest and the baseline helps understand the contributions of each feature.\n",
    "\n",
    "**instance_output_value:** This is the model's output value for the instance we passed in for explanation. In the context of a binary classifier, this can be interpreted as the probability of the instance belonging to the positive class.\n",
    "\n",
    "**feature_attributions_dict:**\n",
    "\n",
    "* **'dense_input':** This is the name of the input tensor to the model. \n",
    "\n",
    "* **The list of numbers: [-0.1632179390639067, 0.0, ...]** represents the importance or attribution of each corresponding feature in the input for the given prediction. The length of this list matches the number of features in our model's input.\n",
    "\n",
    "    * Each number represents the marginal contribution of that feature towards the model's prediction for the specific instance we're explaining, relative to the baseline. In essence, how much did this feature move the prediction from the average/baseline prediction?\n",
    "\n",
    "    * Positive values indicate that the feature pushed the model's output in the positive class's direction. For binary classification, this usually means it made the model more confident in classifying the instance as the positive class.\n",
    "\n",
    "    * Negative values indicate that the feature pushed the model's output in the negative class's direction.\n",
    "\n",
    "    * Zero or close to zero suggests that the feature didn't have a significant impact on the prediction for this particular instance.\n",
    "\n",
    "**approximation_error:** This is the error in the approximation used to compute the attribution values. Explanation methods typically use approximations to compute attributions. The approximation error gives an idea of the confidence we can have in the attribution values – a smaller error typically indicates more reliable attributions.\n",
    "\n",
    "**output_name:** This is the name of the model's output tensor. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "803b6c9c-c1e1-46e5-9556-98091d93ddee",
   "metadata": {},
   "source": [
    "# Lineage tracking "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d092d68-2c98-4143-a9f8-25a0886a0186",
   "metadata": {},
   "source": [
    "The following pieces of code will list the artifacts, executions, and contexts in our GCP project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "808ff8fd-1758-473a-b627-572b1ba6edd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "aiplatform.Artifact.list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4068621-9fd2-4e4e-968a-0fc9053ef79b",
   "metadata": {},
   "outputs": [],
   "source": [
    "aiplatform.Execution.list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4578a30-aa3c-499f-8ba0-dbedd9fab2bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "aiplatform.Context.list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ba9d62-2a0f-404d-910b-c2c8b6d95bc8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-13.m112",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-13:m112"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
