{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd803334-3152-48e4-ba84-3d2a0b6e23c8",
   "metadata": {},
   "source": [
    "# Working with the BigQuery DataFrames Python API\n",
    "\n",
    "The BigQuery DataFrames Python API enables us to use Python to analyze and manipulate data in BigQuery, and perform various machine learning tasks. It’s a relatively new, open-source option launched and maintained by Google Cloud for using dataframes to interact with BigQuery, and we can access it by using the bigframes Python library, which consists of two main parts:\n",
    "•\tbigframes.pandas, which implements a pandas-like API on top of BigQuery.\n",
    "•\tbigframes.ml, which implements a scikit-learn-like API on top of BigQuery ML.\n",
    "\n",
    "This notebook focuses on using **bigframes.pandas**.\n",
    "\n",
    "**Let's begin by importing bigframes.pandas into our notebook (note: this assumes that you are using the bigframes custom Jupyter kernel created during the prerequisite steps in Chapter 14)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "**Note:** This notebook and repository are supporting artifacts for the \"Google Machine Learning and Generative AI for Solutions Architects\" book. The book describes the concepts associated with this notebook, and for some of the activities, the book contains instructions that should be performed before running the steps in the notebooks. Each top-level folder in this repo is associated with a chapter in the book. Please ensure that you have read the relevant chapter sections before performing the activities in this notebook.\n",
    "\n",
    "**There are also important generic prerequisite steps outlined [here](https://github.com/PacktPublishing/Google-Machine-Learning-for-Solutions-Architects/blob/main/Prerequisite-steps/Prerequisites.ipynb).**\n",
    "\n",
    "**Let's begin by importing bigframes.pandas into our notebook (note: this assumes that you are using the bigframes custom Jupyter kernel created during the prerequisite steps in Chapter 14)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e95821d-7f47-44e4-9959-b45d38570967",
   "metadata": {},
   "source": [
    "**Attention:** The code in this notebook creates Google Cloud resources that can incur costs.\n",
    "\n",
    "Refer to the Google Cloud pricing documentation for details.\n",
    "\n",
    "For example:\n",
    "\n",
    "* [Vertex AI Pricing](https://cloud.google.com/vertex-ai/pricing)\n",
    "* [BigQuery Pricing](https://cloud.google.com/bigquery/pricing)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b7155f-6764-4325-90e7-cf9219476063",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import bigframes.pandas as bpd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a685d5de-09f8-4387-8b88-26b73cfc8380",
   "metadata": {},
   "source": [
    "## Define constants\n",
    "\n",
    "Next, we define the constants to contain our project ID and the dataset ID at which we will save our data in BigQuery later.\n",
    "\n",
    "We will use the `gcloud` command to get the Project ID details from the local Google Cloud project, and assign the results to the PROJECT_ID variable. If, for any reason, PROJECT_ID is not set, you can set it manually or change it, if preferred.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d65f7856-04b5-4bb2-b17b-f1b600bdef49",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "PROJECT_ID_DETAILS = !gcloud config get-value project\n",
    "PROJECT_ID = PROJECT_ID_DETAILS[0]  # The project ID is item 0 in the list returned by the gcloud command\n",
    "UPDATED_DATASET_ID = \"new_york_taxi_trips\"\n",
    "TABLE = \"transformed_taxi_data_bigframes\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a774db84-45f4-4609-b7f5-a8b4537b6867",
   "metadata": {},
   "source": [
    "## Read data in from BigQuery\n",
    "\n",
    "The code in the next cell will read data from the `New York Taxi Trips` BigQuery Public Dataset into a dataframe that we can then use in the remaining steps in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7987b90-02b0-4faf-8c8a-4f981f7b830b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = bpd.read_gbq(\"SELECT * FROM `bigquery-public-data.new_york_taxi_trips.tlc_yellow_trips_2020`\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dbd75c8-d2da-4e75-bb68-661a511ba0e6",
   "metadata": {},
   "source": [
    "## Data exploration\n",
    "\n",
    "Now that we've read the data into a dataframe, we can begin to explore our dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8530a10-7796-4d2e-95c7-7b6034f35240",
   "metadata": {},
   "source": [
    "### Preview the data\n",
    "\n",
    "Let's take a look at some of the values in our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02305a79-76a0-4c3d-8138-ebeeaa88e502",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d40bd4-03b6-4874-b4c9-0fe425fb816c",
   "metadata": {},
   "source": [
    "### Explore the data types \n",
    "\n",
    "We can use the dtypes property to explore the data types in the fields of our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4821ec02-2a2b-4c21-847a-b86092454b90",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9301b2bb-caa0-4b2a-abb8-d0e4f48f98a2",
   "metadata": {},
   "source": [
    "### Summary statistics\n",
    "\n",
    "We can use the describe() function to display some summary statistics about the fields in our dataset. This can help us to understand the scale of features in our dataset, by displaying statistics such as `count`, `min`, `max`, `mean`, and the standard deviation (`std`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe046cd-1a7a-476e-b7ba-5486395d91a7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ea2324f-5356-415a-8934-d7f4f1712fbd",
   "metadata": {},
   "source": [
    "### Explore missing values\n",
    "\n",
    "Missing values can cause problems for many machine learning algorithms, so it's often important for data scientists to be aware of any missing values that exist in the dataset, and to address them accordingly. The code in the next cell will tell us how many missing values exist for each feature in the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f9d9528-9e11-4f80-90a0-aca7bb816949",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b41fcd9-ce4a-41bd-9121-8cd3e7eeab01",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Value counts\n",
    "\n",
    "It's also often important to understand how many unique values each feature contains. This is referred to as the `cardinality` of a feature, where low cardinality features have a small number of unique values (e.g., binary features that are either `yes` or `no`), and high cardinality have a large number of unique values (e.g., product IDs).\n",
    "\n",
    "Feature cardinality can be important to understand for tasks such as feature encoding and feature selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f468ca9-0f68-4b1f-a1cb-3387e36e77cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df['passenger_count'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1724584b-2a46-45bf-b650-4956bc3c7f03",
   "metadata": {},
   "source": [
    "## Feature engineering\n",
    "\n",
    "After exploring our data, we can perform any feature engineering that we believe could be important for helping our models to learn specific patterns in our dataset.\n",
    "\n",
    "For example, we can engineer a new feature named `fare_per_mile` by diving the `fare_amount` feature by the `trip_distance` feature, and this new feature may be more useful if we want to build a model that estimates the fare for a given trip distance.\n",
    "\n",
    "To avoid errors such as type mismatches during our division operation, we will convert all types to Float64.\n",
    "To avoid division by zero, we replace all instances of zero in `trip_distance` with `numpy.finfo.eps` (epsilon), which is a tiny positive number. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a44a29-6a1e-4b48-a723-dd77594d7f9c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df['fare_amount'] = df['fare_amount'].astype('Float64')\n",
    "df['trip_distance'] = df['trip_distance'].astype('Float64')\n",
    "df['fare_per_mile'] = df['fare_amount'] / df['trip_distance'].replace(0, np.finfo(float).eps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ffa3bbe-3049-428b-af7f-4681f821090f",
   "metadata": {},
   "source": [
    "We already covered feature engineering extensively in Chapter 7 of the book, and you can refer to the [feature-eng-titanic.ipynb](https://github.com/PacktPublishing/Google-Machine-Learning-for-Solutions-Architects/blob/main/Chapter-07/feature-eng-titanic.ipynb) Jupyter Notebook file for additional examples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c15663-010f-446c-a126-265352b5f9d5",
   "metadata": {},
   "source": [
    "## Writing data to BigQuery\n",
    "\n",
    "After performing our feature engineering steps, we can write our updated data back to BigQuery for long term storage, reference, and analytics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba6e3f0-1266-45ff-aa55-e1aa30e4e5e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_gbq(f\"{PROJECT_ID}.{UPDATED_DATASET_ID}.{TABLE}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e8798f1-2ad2-4399-933d-31c2ec49417c",
   "metadata": {},
   "source": [
    "**After that operation completes, you can view the dataset in the [BigQuery console](https://console.cloud.google.com/bigquery)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba6c612f",
   "metadata": {},
   "source": [
    "# That's it! Well Done!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19d74015",
   "metadata": {},
   "source": [
    "# Clean up\n",
    "\n",
    "When you no longer need the resources created by this notebook. You can delete them as follows.\n",
    "\n",
    "**Note: if you do not delete the resources, you will continue to pay for them.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24ff8d6b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "clean_up = False  # Set to True if you want to delete the resources"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22cf7da1-b4de-494d-9130-b62d53041a18",
   "metadata": {},
   "source": [
    "## Delete BigQuery resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cbc0c71-2676-4dc1-8e4f-c8262193f882",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if clean_up:  \n",
    "    try:\n",
    "        ! bq rm -r -f -d $PROJECT_ID:$UPDATED_DATASET_ID\n",
    "        print(f\"Deleted dataset {UPDATED_DATASET_ID}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error deleting dataset: {e}\")\n",
    "else:\n",
    "    print(\"clean_up parameter is set to False.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "290b954f-9cd1-4cc9-8066-3951b75f7cd0",
   "metadata": {},
   "source": [
    "**You can also verify or delete the dataset in the [BigQuery console](https://console.cloud.google.com/bigquery)**"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-env-bigframes-env-py",
   "name": "workbench-notebooks.m121",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m121"
  },
  "kernelspec": {
   "display_name": "Python 3 (bigframes) (Local)",
   "language": "python",
   "name": "conda-env-bigframes-env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
