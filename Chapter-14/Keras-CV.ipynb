{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29f4c3cd-4a14-4ca4-9d95-5e05a90677b0",
   "metadata": {},
   "source": [
    "# Building a Convolutional Neural Network (CNN) with Keras\n",
    "\n",
    "In this notebnook, we will use Keras to build a Convolutional Neural Network (CNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b081839c-d0ee-4de6-8b5a-9b5f214295c8",
   "metadata": {},
   "source": [
    "## Import libraries\n",
    "\n",
    "We start by importing the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b7d75f-e3fa-4781-8efe-76b1f7012c72",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, datasets, optimizers, utils\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b00888d1-c685-4dcc-9067-4529544b8599",
   "metadata": {},
   "source": [
    "## Check if GPU is available and set the device accordingly\n",
    "\n",
    "We have attached a GPU to our Vertex AI Notebook Instance. In order to make our code more flexible, we will check for the presence of a GPU. If a GPU is present, we will set it as our training device; otherwise, we'll specify CPU as our training device.\n",
    "\n",
    "You can try playing around with this code to see if you notice any differences when training on CPU vs GPU. Bear in mind that GPU's are designed to handle paralellizable tasks more efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e3f442-3111-49c4-9640-f5680a26719e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if tf.config.list_physical_devices('GPU'):\n",
    "    device = '/GPU:0'\n",
    "else:\n",
    "    device = '/CPU:0'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "630b748f-18f4-493a-a0e1-f50b4d0d7ff4",
   "metadata": {},
   "source": [
    "## Load and process the dataset\n",
    "\n",
    "In the next cell, we wil load the [CIFAR-10 dataset](https://keras.io/api/datasets/cifar10/), which is a dataset that consists of 60000 32x32 colour images in 10 classes, with 6000 images per class. There are 50000 training images and 10000 test images. \n",
    "\n",
    "The classes are: 'airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', and 'truck'.\n",
    "\n",
    "The code in the following cell will loads the dataset into four variables:\n",
    "* x_train: Training images (50,000 images, each represented as a 32x32x3 array of pixel values).\n",
    "* y_train: Training labels (50,000 labels, each an integer representing the class of the corresponding image, from 0 to 9).\n",
    "* x_test: Test images (10,000 images, with the same structure as x_train).\n",
    "* y_test: Test labels (10,000 labels, with the same structure as y_train)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa05d896-52ea-4327-9ebe-8d274b690ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = datasets.cifar10.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c0d122-96b6-49c7-a23a-1a6543c0bc1d",
   "metadata": {},
   "source": [
    "### Normalize the pixel values of the images\n",
    "\n",
    "Next, we will normalize the pixel values of both the training and test images to a range between 0 and 1. \n",
    "(Pixel values in images usually range from 0 to 255, so dividing by 255 scales them to 0-1.)\n",
    "\n",
    "This is important because:\n",
    "1. Neural networks often work better with normalized input data.\n",
    "1. It prevents features with larger ranges from dominating the learning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "651306e2-7905-4aaf-876e-f57f433d6373",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test = x_train / 255.0, x_test / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed4b39d0-e898-4dd8-81a0-0d336727aa7d",
   "metadata": {},
   "source": [
    "### Perform one-hot encoding on our class labels\n",
    "\n",
    "Next, we will perform one-hot encoding to convert the class labels (which are represented as integers from 0 to 9 in the source dataset) into binary class matrices. \n",
    "\n",
    "This means that each label is represented as a 10-dimensional vector with a 1 in the position corresponding to the class, and 0s elsewhere (e.g., the label \"3\" would become [0, 0, 0, 1, 0, 0, 0, 0, 0, 0]). This format is more suitable for neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6bbbcb8-e947-473b-842d-ffcf09d1896c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Convert class vectors to binary class matrices\n",
    "y_train = utils.to_categorical(y_train, 10)\n",
    "y_test = utils.to_categorical(y_test, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "687f1ea4-07c3-434c-ae89-79e0c8064fe3",
   "metadata": {},
   "source": [
    "## Define our CNN model\n",
    "\n",
    "The code in the next cell will define a CNN with the following layers (see the descriptions in the text in Chapter-14 in our book for reference):\n",
    "\n",
    "* **Convolutional layers:** These layers extract features from the input images using filters. \n",
    "* **Max pooling layers:** These layers reduce the dimensionality of the features, making the model more efficient and less prone to overfitting. We also downsample the input by taking the maximum value in 2x2 patches.\n",
    "* **Flattening layer:** This layer flattens the 3D output of the convolutional layers into a 1D vector, suitable for input to the dense layers.\n",
    "* **Dense layers:** These layers perform the final classification. We define one dense layer with 64 neurons and ReLU activation, and then an output layer with 10 neurons (one for each class) and softmax activation to produce probability scores. This is what provides the probability at which the input image was a member of each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "095f81e7-61c4-4fb9-8321-1d240433ad65",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the CNN model\n",
    "def create_model():\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(64, activation='relu'))\n",
    "    model.add(layers.Dense(10, activation='softmax'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a9effb-9103-46f4-8e5a-bc33cc3ba55d",
   "metadata": {},
   "source": [
    "## Train our CNN model\n",
    "\n",
    "The code in the next cell will perform the following steps:\n",
    "\n",
    "1. Set up the context manager for device placement: The line specifies a context manager to ensure that the subsequent code is executed on the specified device (i.e., CPU or GPU, based on the check we performed at the beginning of this notebook).\n",
    "1. Call the previously defined create_model() function to build the CNN model architecture. It assigns the resulting model object to the variable `net`.\n",
    "1. Configure the training process with the following parameters:\n",
    "* Optimizer: This sets the optimizer to be used during training. Here, we use the Stochastic Gradient Descent (SGD) optimizer with a learning rate of 0.001 and momentum of 0.9.\n",
    "* Loss function: This sets the loss function to be minimized during training. Here, we use categorical crossentropy, which is a common choice for multi-class classification problems.\n",
    "* Metrics: This specifies the metrics to be monitored during training and evaluation. Here, we use `accuracy`, which measures the percentage of correctly classified examples.\n",
    "4. Train the model (using `net.fit()` and specifying the required configurations, such as input and validation datasets, batch size, and number of epochs).\n",
    "5. Save the resulting model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff87129-473c-4add-857b-5af03a08d6c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with tf.device(device):\n",
    "    net = create_model()\n",
    "\n",
    "    # Compile the model\n",
    "    net.compile(optimizer=optimizers.SGD(learning_rate=0.001, momentum=0.9),\n",
    "                loss='categorical_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "    # Train the network\n",
    "    history = net.fit(x_train, y_train, epochs=20, batch_size=4,\n",
    "                      validation_data=(x_test, y_test))    \n",
    "\n",
    "# Save the trained model\n",
    "net.save('cifar_net.keras')   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "855384fb-9940-42d9-8d43-be30ff825647",
   "metadata": {},
   "source": [
    "## Evaluate our model\n",
    "\n",
    "Next, we load our trained model and use the `net.evaluate` function to evaluate the model against the test dataset.\n",
    "Finally, we print the accuracy score from the evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ab44ad-2aad-4068-aae7-7a02c6eb63cf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the saved model\n",
    "net = models.load_model('cifar_net.keras')\n",
    "\n",
    "# Evaluate the model on the test dataset\n",
    "test_loss, test_acc = net.evaluate(x_test, y_test, verbose=2)\n",
    "print(f'Accuracy of the network on the 10000 test images: {test_acc * 100} %')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e1dc0b9-8853-4109-95e7-c910ada0bc1f",
   "metadata": {},
   "source": [
    "## Get predictions from our model\n",
    "\n",
    "The code in the next cell will perform the following steps:\n",
    "\n",
    "1. Define the classes for mapping to numerical representations in source dataset ('airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "1. Define a function to display images from the dataset. This function takes an image as input and displays it using Matplotlib's imshow function.. It also rescales pixel values from 0-1 to 0-255 for visualization.\n",
    "1. Get some random testing images and labels\n",
    "1. Print the images\n",
    "1. Predict labels for the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f6e74ba-4f4b-48b9-b635-b04b2746923f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Define the classes for mapping to numerical representations in source dataset\n",
    "classes = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "\n",
    "# Function to display images\n",
    "def imshow(img):\n",
    "    img = img * 255\n",
    "    plt.imshow(img.astype('uint8'))\n",
    "    plt.show()\n",
    "\n",
    "# Get some random testing images and labels\n",
    "n = 4  # Number of images to display\n",
    "idx = np.random.choice(x_test.shape[0], n, replace=False) # Randomly select n indices from the test set without replacement.\n",
    "images, labels = x_test[idx], y_test[idx] # Retrieves the images and labels at those indices.\n",
    "\n",
    "# Print images\n",
    "imshow(np.hstack(images)) # Concatenates the images horizontally to display them side-by-side.\n",
    "print('GroundTruth: ', ' '.join(classes[np.argmax(label)] for label in labels)) # Finds the class with the highest probability for each label, and joins the class names into a string separated by spaces.)\n",
    "\n",
    "\n",
    "# Use the trained model to predict the classes for the selected images.\n",
    "predicted = net.predict(images) \n",
    "predicted_classes = np.argmax(predicted, axis=1) # Finds the index of the highest probability class for each prediction.\n",
    "\n",
    "# Convert the predicted indices back to class names and joins the class names into a string for printing.\n",
    "print('Predicted: ', ' '.join(classes[cls] for cls in predicted_classes)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa8ec37-2c0e-4bb6-a328-483f5aaf4beb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-env-tensorflow-tensorflow",
   "name": "workbench-notebooks.m113",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/workbench-notebooks:m113"
  },
  "kernelspec": {
   "display_name": "TensorFlow 2-11 (Local)",
   "language": "python",
   "name": "conda-env-tensorflow-tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
