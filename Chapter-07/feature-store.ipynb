{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f88b52e5-275a-4a23-8027-67cf8a3f2f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2023 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4af5a2a-9971-4028-bbe9-aa38fac101a5",
   "metadata": {},
   "source": [
    "# Vertex AI Feature Store\n",
    "\n",
    "In this notebook, we will build and use a [Vertex AI Feature Store](https://cloud.google.com/vertex-ai/docs/featurestore).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "**Note:** This notebook and repo are supporting artifacts for the \"Google Machine Learning and Generative AI for Solutions Architects\" book. The book describes the concepts associated with this notebook, and for some of the activities, the book contains instructions that should be performed before running the steps in the notebooks. Each top-level folder in this repo is associated with a chapter in the book. Please ensure that you have read the relevant chapter sections before performing the activities in this notebook.\n",
    "\n",
    "**There are also important generic prerequisite steps outlined [here](https://github.com/PacktPublishing/Google-Machine-Learning-for-Solutions-Architects/blob/main/Prerequisite-steps/Prerequisites.ipynb).**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e95821d-7f47-44e4-9959-b45d38570967",
   "metadata": {},
   "source": [
    "**Attention:** The code in this notebook creates Google Cloud resources that can incur costs.\n",
    "\n",
    "Refer to the Google Cloud pricing documentation for details.\n",
    "\n",
    "For example:\n",
    "\n",
    "* [Vertex AI Pricing](https://cloud.google.com/vertex-ai/pricing)\n",
    "* [BigQuery Pricing](https://cloud.google.com/bigquery/pricing)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "389bfe78-0074-4d95-be1b-b4da8b021e1e",
   "metadata": {},
   "source": [
    "## Install and import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b0cbe98-22b6-4802-89e4-21c607b6de54",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! pip install --upgrade --quiet google-cloud-bigquery google-cloud-aiplatform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc501e5-d6f5-45fb-adf8-f35c049c52f0",
   "metadata": {},
   "source": [
    "## Restart the kernel\n",
    "\n",
    "The code in the next cell will retart the kernel, which is sometimes required after installing/upgrading packages.\n",
    "\n",
    "When prompted, click OK to restart the kernel.\n",
    "\n",
    "The sleep command simply prevents further cells from executing before the kernel restarts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "194c7eda-c3f8-4f97-92a4-68e6dbde5f41",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import IPython\n",
    "import time\n",
    "\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(True)\n",
    "\n",
    "time.sleep(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c023cc1b-0c2a-45fd-a4e2-db5e719c723f",
   "metadata": {},
   "source": [
    "# (Wait for kernel to restart before proceeding...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f2813fb-d798-4d22-b6b1-40f578b35da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import bigquery\n",
    "from google.cloud.aiplatform_v1beta1 import FeatureOnlineStoreAdminServiceClient, FeatureOnlineStoreServiceClient, FeatureRegistryServiceClient\n",
    "from google.cloud.aiplatform_v1beta1.types import feature as feature_pb2\n",
    "from google.cloud.aiplatform_v1beta1.types import feature_registry_service as feature_registry_service_pb2\n",
    "from google.cloud.aiplatform_v1beta1.types import featurestore_service as featurestore_service_pb2\n",
    "from google.cloud.aiplatform_v1beta1.types import feature_online_store_service as feature_online_store_service_pb2\n",
    "from google.cloud.aiplatform_v1beta1.types import feature_group as feature_group_pb2\n",
    "from google.cloud.aiplatform_v1beta1.types import feature_online_store as feature_online_store_pb2\n",
    "from google.cloud.aiplatform_v1beta1.types import feature_online_store_admin_service as feature_online_store_admin_service_pb2\n",
    "from google.cloud.aiplatform_v1beta1.types import feature_view as feature_view_pb2\n",
    "from google.cloud.aiplatform_v1beta1.types import io as io_pb2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "435dcd86-86b3-4186-88b7-365fa35f0f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID_DETAILS = !gcloud config get-value project\n",
    "PROJECT_ID = PROJECT_ID_DETAILS[0]  # The project ID is item 0 in the list returned by the gcloud command\n",
    "REGION=\"us-central1\"\n",
    "API_ENDPOINT = f\"{REGION}-aiplatform.googleapis.com\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b91122-3da6-4d5c-b536-cdb06a9be2e2",
   "metadata": {},
   "source": [
    "# Access and explore the Data: \n",
    "\n",
    "The NYC Taxi Trip Records dataset is available on BigQuery as a public dataset.\n",
    "In the next cell, we will:\n",
    "Familiarize ourselves with the data: Understand the dataset's structure, including the available fields like pick-up/drop-off times and locations, distances, fares, etc.\n",
    "Identify Key Features: Determine which features are relevant for our use case. For example, average trip duration, total trips per day, average fare, peak hours, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa57baf-3d95-421a-8d90-23d442382f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a BigQuery client\n",
    "client = bigquery.Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "067c50e2-4f1e-4c11-baed-7911f585c259",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the public dataset and table\n",
    "public_dataset_name = 'new_york_taxi_trips'\n",
    "public_project_name = 'bigquery-public-data'\n",
    "public_table_name = 'tlc_yellow_trips_2020'\n",
    "\n",
    "# Prepare a query to identify key features\n",
    "query = f\"\"\"\n",
    "SELECT\n",
    "    EXTRACT(HOUR FROM pickup_datetime) AS hour,\n",
    "    AVG(trip_distance) AS avg_trip_distance,\n",
    "    COUNT(*) AS total_trips,\n",
    "    AVG(fare_amount) AS avg_fare,\n",
    "    AVG((fare_amount / NULLIF(trip_distance, 0))) AS avg_fare_per_mile\n",
    "FROM\n",
    "    `{public_project_name}.{public_dataset_name}.{public_table_name}`\n",
    "WHERE\n",
    "    trip_distance > 0 AND fare_amount > 0\n",
    "GROUP BY\n",
    "    hour\n",
    "ORDER BY\n",
    "    hour\n",
    "\"\"\"\n",
    "\n",
    "# Run the query\n",
    "query_job = client.query(query)\n",
    "\n",
    "# Print the results to understand key features\n",
    "print(\"Hourly Trip Data:\")\n",
    "for row in query_job:\n",
    "    print(f\"Hour: {row.hour}, Avg Trip Distance: {row.avg_trip_distance}, Total Trips: {row.total_trips}, Avg Fare: {row.avg_fare}, Avg Fare per Mile: {row.avg_fare_per_mile}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d30c74c2-028c-4040-96e5-6e68f270d3e7",
   "metadata": {},
   "source": [
    "# Data Preparation and Feature Engineering\n",
    "\n",
    "The following python code will perform:\n",
    "\n",
    "Data Extraction: Selects relevant columns from the taxi trip data.\n",
    "\n",
    "Feature Engineering:\n",
    "1. Calculates fare_per_mile by dividing the fare amount by the trip distance.\n",
    "1. Extracts pickup_hour and pickup_day_of_week from the pickup datetime.\n",
    "1. Extracts dropoff_hour and dropoff_day_of_week from the dropoff datetime.\n",
    "1. Filters out records with zero or negative trip distances or fares.\n",
    "1. Limits the query to 1000 rows for initial analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc323b81-6c7e-4bde-a53f-7cb64c298e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the query\n",
    "query = f\"\"\"\n",
    "SELECT\n",
    "    *,\n",
    "    (fare_amount / NULLIF(trip_distance, 0)) AS fare_per_mile,\n",
    "    EXTRACT(HOUR FROM pickup_datetime) AS pickup_hour,\n",
    "    EXTRACT(DAYOFWEEK FROM pickup_datetime) AS pickup_day_of_week,\n",
    "    EXTRACT(HOUR FROM dropoff_datetime) AS dropoff_hour,\n",
    "    EXTRACT(DAYOFWEEK FROM dropoff_datetime) AS dropoff_day_of_week\n",
    "FROM\n",
    "    `{public_project_name}.{public_dataset_name}.{public_table_name}`\n",
    "WHERE\n",
    "    trip_distance > 0 AND\n",
    "    fare_amount > 0\n",
    "LIMIT 1000\n",
    "\"\"\"\n",
    "\n",
    "# Run the query and get the results\n",
    "query_job = client.query(query)\n",
    "\n",
    "# Print the first 5 rows to verify the results\n",
    "row_count = 0\n",
    "for row in query_job:\n",
    "    print(row)\n",
    "    row_count += 1\n",
    "    if row_count >= 5:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a05a349d-3d92-41ea-91e2-10056e47c6a1",
   "metadata": {},
   "source": [
    "# Create dataset to store features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab074fb-fde6-477e-b67f-f748379bb674",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your dataset ID (replace with your desired dataset name)\n",
    "feature_dataset_name = 'feature_store_for_nyc_taxi_data'\n",
    "dataset_id = f\"{PROJECT_ID}.{feature_dataset_name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb4565a-c96d-4b02-ac88-d46f6c8b2cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Dataset\n",
    "dataset = bigquery.Dataset(dataset_id)\n",
    "dataset.location = \"US\"  # Choose your location, e.g., \"US\", \"EU\", etc.\n",
    "client.create_dataset(dataset, exists_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00cda429-397a-4489-933e-884b1565bc84",
   "metadata": {},
   "source": [
    "## Create a view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd21b5d-5cfe-444c-929e-55b8898db300",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our dataset and view name\n",
    "view_name = 'nyc_taxi_data_view'\n",
    "view_id = f\"{dataset_id}.{view_name}\"\n",
    "\n",
    "# Define the fully qualified name of your BigQuery view\n",
    "BQ_VIEW_ID_FQN = f\"bq://{view_id}\"\n",
    "\n",
    "# SQL query to define the view\n",
    "view_query = f\"\"\"\n",
    "SELECT\n",
    "  pickup_datetime,\n",
    "  dropoff_datetime,\n",
    "  passenger_count,\n",
    "  CAST(trip_distance AS FLOAT64) AS trip_distance,  -- Cast to FLOAT64 (supported by Feature Store)\n",
    "  CAST(fare_amount AS FLOAT64) AS fare_amount,      -- Cast to FLOAT64 (supported by Feature Store)\n",
    "  CAST((fare_amount / NULLIF(trip_distance, 0)) AS FLOAT64) AS fare_per_mile, -- Cast to FLOAT64 (supported by Feature Store)\n",
    "  EXTRACT(HOUR FROM pickup_datetime) AS pickup_hour,\n",
    "  EXTRACT(DAYOFWEEK FROM pickup_datetime) AS pickup_day_of_week,\n",
    "  EXTRACT(HOUR FROM dropoff_datetime) AS dropoff_hour,\n",
    "  EXTRACT(DAYOFWEEK FROM dropoff_datetime) AS dropoff_day_of_week,\n",
    "  CONCAT(pickup_datetime, '-', CAST(pickup_location_id AS STRING)) AS entity_id,\n",
    "  pickup_datetime AS feature_timestamp\n",
    "FROM\n",
    "  `{public_project_name}.{public_dataset_name}.{public_table_name}`\n",
    "WHERE\n",
    "  trip_distance > 0 AND fare_amount > 0\n",
    "\"\"\"\n",
    "\n",
    "# Create the view\n",
    "view = bigquery.Table(view_id)\n",
    "view.view_query = view_query\n",
    "client.create_table(view, exists_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8edddf2e-48b1-477c-b38a-c4722740d084",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = f\"\"\"\n",
    "SELECT DISTINCT entity_id\n",
    "FROM `{view_id}`\n",
    "LIMIT 100  -- Adjust the limit as needed\n",
    "\"\"\"\n",
    "\n",
    "# Run the query\n",
    "query_job = client.query(query)\n",
    "\n",
    "# Initialize an empty list to store entity IDs\n",
    "entity_ids = []\n",
    "\n",
    "# Fetch and print the results, and add each entity_id to the list\n",
    "for row in query_job:\n",
    "    entity_ids.append(row.entity_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0912c67c-4e73-459f-a8c8-c943f0c223e9",
   "metadata": {},
   "source": [
    "## Set up and start online serving\n",
    "\n",
    "Some of the cells in this section contain repurposed and/or modified code from [this example notebook](https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/feature_store/online_feature_serving_and_fetching_bigquery_data_with_feature_store_optimized.ipynb).\n",
    "\n",
    "Now for the exciting part! To serve data in a feature store, you need to do the following:\n",
    "\n",
    "1. Create an online store cluster to host the data.\n",
    "    * Create a `FeatureOnlineStore` instance with autoscaling.\n",
    "    * Choose Optimized as the storage type.\n",
    "1. Define the data (`FeatureView`) to be served by the newly-created instance. This can map to either of the following:\n",
    "    * The BigQuery view that you just created for serving data.\n",
    "    * The `FeatureGroup` and `Feature` we will create to host feature metadata.\n",
    "\n",
    "We recommend NOT sending loads larger than 7500 QPS to one FeatureOnlineStore.\n",
    "In general, we recommend creating multiple gRPC connections to one FeatureOnlineStore, and evenly distribute your loads across them. More connections and smaller per-connection QPS typically help with internal load balancing and scaling, reducing the chance of seeing higher tail-latencies. Specifically:\n",
    "1. If your FetchFeatureValues response payload size is small (e.g. less than 1 kB), you may create one connection for up to every 2000 QPS.\n",
    "2. If your FetchFeatureValues response payload size can be large (e.g. more than a few kB or 10s of kB), we recommend you to create one connection for up to every 250 QPS, and we also recommend avoiding sudden increases of loads.\n",
    "\n",
    "## Initialize the admin and registry clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a005abe2-17db-435b-b3cf-6ab308d91a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "admin_client = FeatureOnlineStoreAdminServiceClient(\n",
    "    client_options={\"api_endpoint\": API_ENDPOINT}\n",
    ")\n",
    "registry_client = FeatureRegistryServiceClient(\n",
    "    client_options={\"api_endpoint\": API_ENDPOINT}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c64532d9-183b-4e9d-aff6-d5c23816fc91",
   "metadata": {},
   "source": [
    "## Create online store instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e03f3237-ea13-4dd4-87bd-80c8a983934a",
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURE_ONLINE_STORE_ID = (\n",
    "    \"feature_store_nyc_taxi_online\"  # @param {type:\"string\"}\n",
    ")\n",
    "online_store_config = feature_online_store_pb2.FeatureOnlineStore(\n",
    "    optimized=feature_online_store_pb2.FeatureOnlineStore.Optimized()\n",
    ")\n",
    "\n",
    "\n",
    "create_store_lro = admin_client.create_feature_online_store(\n",
    "    feature_online_store_admin_service_pb2.CreateFeatureOnlineStoreRequest(\n",
    "        parent=f\"projects/{PROJECT_ID}/locations/{REGION}\",\n",
    "        feature_online_store_id=FEATURE_ONLINE_STORE_ID,\n",
    "        feature_online_store=online_store_config,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f27ef3-e709-4bc2-b0fe-ce9675626cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wait for the LRO to finish and get the LRO result.\n",
    "print(create_store_lro.result())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef2773e-a539-4309-a249-f476fab66d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use list to verify the store is created.\n",
    "admin_client.list_feature_online_stores(\n",
    "    parent=f\"projects/{PROJECT_ID}/locations/{REGION}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41bf85d4-ea22-407d-a7dc-3f432285fe03",
   "metadata": {},
   "source": [
    "## Create FeatureGroup/Features\n",
    "\n",
    "Create a FeatureGroup pointing to the created BigQuery view for the demo. We will then create features for each column we would like to register.\n",
    "\n",
    "##### Data source preparation guidelines for Feature Registry data source\n",
    "\n",
    "Note that if you choose to use Feature Registry source, Feature Store only provides the option to support time-series sources for which Feature Store will generate latest featureValues.\n",
    "\n",
    "Use the following guidelines to understand the schema and constraints while creating the BigQuery source:\n",
    "\n",
    "* The BigQuery table or view *must* have a column with `string` values to use as the (entity) IDs. You'll need to specify that this column is the ID column during `FeatureGroup` creation. Note that the size of each value in this column must be less than 4 KB.\n",
    "* The BigQuery table or view *must* have a column named `feature_timestamp` with `timestamp` values to use as timestamp column.\n",
    "* Feature Registry sources are treated as sparse by default i.e. a point in time lookup (BQ.ML_FEATURES_AT_TIME()) to generate latest featureValues per entityId.\n",
    "* Provide values for each feature is a separate column. Supported data types are `bool`, `int`, `double`, `string`, timestamp, arrays of these data types, and bytes. Note that the timestamp data type is converted to `int64` during data sync.\n",
    "* Feature Store validates the schema during `FeatureView`/`FeatureGroup`/`Featre` creation. However, it doesn't revalidate the schema during each data sync. Columns with unsupported data types added after `FeatureView` creation time are ignored.\n",
    "* The BigQuery table or view must be in either the same region as the online store, or in a multiregion that overlaps with the online store. For example, if the online store is in `us-central`, the BigQuery source can be in `us-central` or in `US`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d0b0b26-24ae-4d65-b566-4e8eaa79dc20",
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURE_GROUP_ID = \"feature_group_nyc_taxi\"  # Replace with your feature group ID\n",
    "FEATURE_IDS = [\n",
    "    \"pickup_datetime\",\n",
    "    \"dropoff_datetime\",\n",
    "    \"passenger_count\",\n",
    "    \"trip_distance\",\n",
    "    \"fare_amount\",\n",
    "    \"fare_per_mile\",\n",
    "    \"pickup_hour\",\n",
    "    \"pickup_day_of_week\",\n",
    "    \"dropoff_hour\",\n",
    "    \"dropoff_day_of_week\",\n",
    "]\n",
    "\n",
    "# Create the feature group configuration\n",
    "feature_group_config = feature_group_pb2.FeatureGroup(\n",
    "    big_query=feature_group_pb2.FeatureGroup.BigQuery(\n",
    "        big_query_source=io_pb2.BigQuerySource(input_uri=BQ_VIEW_ID_FQN)\n",
    "    )\n",
    ")\n",
    "\n",
    "# Create the feature group\n",
    "create_group_lro = registry_client.create_feature_group(\n",
    "    feature_registry_service_pb2.CreateFeatureGroupRequest(\n",
    "        parent=f\"projects/{PROJECT_ID}/locations/{REGION}\",\n",
    "        feature_group_id=FEATURE_GROUP_ID,\n",
    "        feature_group=feature_group_config,\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a20b12a-4d03-468e-bb9b-18b279d916a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(create_group_lro.result())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b16751e3-be1a-445b-ba5f-92347fdbd69f",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_feature_lros = []\n",
    "for id in FEATURE_IDS:\n",
    "    create_feature_lros.append(\n",
    "        registry_client.create_feature(\n",
    "            featurestore_service_pb2.CreateFeatureRequest(\n",
    "                parent=f\"projects/{PROJECT_ID}/locations/{REGION}/featureGroups/{FEATURE_GROUP_ID}\",\n",
    "                feature_id=id,\n",
    "                feature=feature_pb2.Feature(),\n",
    "            )\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eff82a5-2323-4c80-8d9d-28229533ab19",
   "metadata": {},
   "outputs": [],
   "source": [
    "for lro in create_feature_lros:\n",
    "    print(lro.result())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a57bd713-8da5-49d1-9fb0-d800f9897a3a",
   "metadata": {},
   "source": [
    "Create a `FeatureView` instance for the BigQuery view and FeatureGroup/features you created earlier in this tutorial and set the sync time and frequency to 1:00 AM PST daily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "915a5915-7916-47bb-8ee9-d23bce818f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURE_VIEW_ID = \"registry_product\"  # @param {type:\"string\"}\n",
    "CRON_SCHEDULE = \"TZ=America/Los_Angeles 56 * * * *\"  # @param {type:\"string\"}\n",
    "\n",
    "feature_registry_source = feature_view_pb2.FeatureView.FeatureRegistrySource(\n",
    "    feature_groups=[\n",
    "        feature_view_pb2.FeatureView.FeatureRegistrySource.FeatureGroup(\n",
    "            feature_group_id=FEATURE_GROUP_ID, feature_ids=FEATURE_IDS\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "sync_config = feature_view_pb2.FeatureView.SyncConfig(cron=CRON_SCHEDULE)\n",
    "\n",
    "create_view_lro = admin_client.create_feature_view(\n",
    "    feature_online_store_admin_service_pb2.CreateFeatureViewRequest(\n",
    "        parent=f\"projects/{PROJECT_ID}/locations/{REGION}/featureOnlineStores/{FEATURE_ONLINE_STORE_ID}\",\n",
    "        feature_view_id=FEATURE_VIEW_ID,\n",
    "        feature_view=feature_view_pb2.FeatureView(\n",
    "            feature_registry_source=feature_registry_source,\n",
    "            sync_config=sync_config,\n",
    "        ),\n",
    "    )\n",
    ")\n",
    "\n",
    "# Wait for LRO to complete and show result\n",
    "print(create_view_lro.result())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f58fa19-a495-4b1c-b44e-2974f4cfb6c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Again, list all feature view under the FEATURE_ONLINE_STORE_ID to confirm\n",
    "admin_client.list_feature_views(\n",
    "    parent=f\"projects/{PROJECT_ID}/locations/{REGION}/featureOnlineStores/{FEATURE_ONLINE_STORE_ID}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0312fb1f-ca9f-4de8-a98e-226378766deb",
   "metadata": {},
   "source": [
    "### Start sync manually\n",
    "\n",
    "The sync pipeline executes according to the schedule specified in the `FeatureView` instance.\n",
    "\n",
    "To skip the wait and execute the sync pipeline immediately, start the sync manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a22c604d-d1c7-40f5-883c-c1eb2abb302f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sync_response = admin_client.sync_feature_view(\n",
    "    feature_view=f\"projects/{PROJECT_ID}/locations/{REGION}/featureOnlineStores/{FEATURE_ONLINE_STORE_ID}/featureViews/{FEATURE_VIEW_ID}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb4d508c-83cb-4975-821b-178f6ea88ec1",
   "metadata": {},
   "source": [
    "The `sync_response` contains the ID of the sync job.\n",
    "\n",
    "Use `get_feature_view_sync` to check the status of the job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d9ee0ae-3039-4b07-a019-3a08f9128ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "while True:\n",
    "    feature_view_sync = admin_client.get_feature_view_sync(\n",
    "        name=sync_response.feature_view_sync\n",
    "    )\n",
    "    if feature_view_sync.run_time.end_time.seconds > 0:\n",
    "        status = \"Succeed\" if feature_view_sync.final_status.code == 0 else \"Failed\"\n",
    "        print(f\"Sync {status} for {feature_view_sync.name}.\")\n",
    "        break\n",
    "    else:\n",
    "        print(\"Sync ongoing, waiting for 30 seconds.\")\n",
    "    time.sleep(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae561410-e763-4ddf-90f1-56955b6883e8",
   "metadata": {},
   "source": [
    "Use `list_feature_view_syncs` to view all your syncs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1947ccdf-98f0-4b42-8391-ba0d809bf932",
   "metadata": {},
   "outputs": [],
   "source": [
    "admin_client.list_feature_view_syncs(\n",
    "    parent=f\"projects/{PROJECT_ID}/locations/{REGION}/featureOnlineStores/{FEATURE_ONLINE_STORE_ID}/featureViews/{FEATURE_VIEW_ID}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "505aac4a-eb39-4b3a-9284-7187bc2a5cc8",
   "metadata": {},
   "source": [
    "### Start online serving\n",
    "\n",
    "After the data sync is complete, use the `FetchFeatureValues` API to retrieve the data. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "063aca7b-3b03-4e7d-986e-ee23212b2270",
   "metadata": {},
   "source": [
    "**Retrieve your features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1efee54-ae80-4227-b744-4e7f39a3ed4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Optimized online store\n",
    "response = admin_client.get_feature_online_store(\n",
    "    name=f\"projects/{PROJECT_ID}/locations/{REGION}/featureOnlineStores/{FEATURE_ONLINE_STORE_ID}\"\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "142438aa-b2b7-467b-addd-d5d342f5d584",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_client = FeatureOnlineStoreServiceClient(\n",
    "    client_options={\n",
    "        \"api_endpoint\": response.dedicated_serving_endpoint.public_endpoint_domain_name\n",
    "    }\n",
    ")\n",
    "print(data_client)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36593bb4-0e5d-4d1c-9999-497a87baa2aa",
   "metadata": {},
   "source": [
    "**Note:** Sometimes it can take a few seconds for the API endpoint to come online to become reachable. The following code will periodically check for the feature view to be available, and will print the status. \n",
    "\n",
    "While waiting for it to become available, you may see expected error messages such as: \n",
    "\n",
    "`503 failed to connect to all addresses; last error: UNAVAILABLE: ipv4:XXX.XXX.XXX.XXX:443: Socket closed`. \n",
    "\n",
    "When you see a message saying **\"Feature view is online and available\"** then you can proceed to the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe23cf3-e6de-46c2-a29b-b30de2c559ee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from google.api_core.exceptions import GoogleAPICallError\n",
    "import time\n",
    "\n",
    "# Define a timeout (e.g., 10 minutes)\n",
    "timeout = 600  # seconds\n",
    "start_time = time.time()\n",
    "\n",
    "while time.time() - start_time < timeout:\n",
    "    try:\n",
    "        # Attempt to fetch feature values (as a test)\n",
    "        test_response = data_client.fetch_feature_values(\n",
    "            request=feature_online_store_service_pb2.FetchFeatureValuesRequest(\n",
    "                feature_view=f\"projects/{PROJECT_ID}/locations/{REGION}/featureOnlineStores/{FEATURE_ONLINE_STORE_ID}/featureViews/{FEATURE_VIEW_ID}\",\n",
    "                id=entity_ids[0], \n",
    "                format=feature_online_store_service_pb2.FetchFeatureValuesRequest.Format.PROTO_STRUCT,\n",
    "            )\n",
    "        )\n",
    "        print(\"Feature view is online and available.\")\n",
    "        break\n",
    "    except GoogleAPICallError as e:\n",
    "        # Handle exceptions related to unavailability or other API errors\n",
    "        print(f\"Waiting for feature view to be available: {e}\")\n",
    "        time.sleep(30)  # Wait for 30 seconds before retrying\n",
    "\n",
    "# Check for timeout\n",
    "if time.time() - start_time >= timeout:\n",
    "    print(\"Timed out waiting for feature view to be available.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98afe2e8-b3ba-4b81-a9ea-ace5fd87e95a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_client.fetch_feature_values(\n",
    "    request=feature_online_store_service_pb2.FetchFeatureValuesRequest(\n",
    "        feature_view=f\"projects/{PROJECT_ID}/locations/{REGION}/featureOnlineStores/{FEATURE_ONLINE_STORE_ID}/featureViews/{FEATURE_VIEW_ID}\",\n",
    "        id=entity_ids[0],\n",
    "        format=feature_online_store_service_pb2.FetchFeatureValuesRequest.Format.PROTO_STRUCT,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dfd0bfc-ede1-455b-98d1-f3b1005b7004",
   "metadata": {},
   "source": [
    "# Train a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d0c0f60-dc24-4489-8be3-14c6521f3812",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Initialize a BigQuery client\n",
    "client = bigquery.Client()\n",
    "\n",
    "# Define your query\n",
    "query = f\"\"\"\n",
    "SELECT \n",
    "  trip_distance, \n",
    "  fare_amount, \n",
    "  EXTRACT(HOUR FROM pickup_datetime) AS pickup_hour,\n",
    "  EXTRACT(DAYOFWEEK FROM pickup_datetime) AS pickup_day_of_week\n",
    "FROM \n",
    "  `{view_id}`\n",
    "\"\"\"\n",
    "\n",
    "# Run the query and convert to a pandas DataFrame\n",
    "df = client.query(query).to_dataframe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "523be175-c827-431e-b83f-31cb4b725b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[['trip_distance', 'pickup_hour', 'pickup_day_of_week']]\n",
    "y = df['fare_amount']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec813c89-bdef-43d8-99e6-ee3c87fb23bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e25b104c-c376-4d14-bd02-cca4c82a7ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Initialize the model\n",
    "model = LinearRegression()\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d927f7e-03b0-461f-8f40-fb790cc04354",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# Calculate Mean Absolute Error (MAE)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "print(f\"Mean Absolute Error: {mae}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cabaae8-0dfc-4fd5-97a4-959944a7fc72",
   "metadata": {},
   "source": [
    "# That's it! Well Done!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afcb76c1-7a07-4d89-ada7-21245c7be5ba",
   "metadata": {},
   "source": [
    "# Clean up\n",
    "\n",
    "When you no longer need the resources created by this notebook. You can delete them as follows.\n",
    "\n",
    "**Note: if you do not delete the resources, you will continue to pay for them.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cefbb9b-3110-4a1f-8e5b-6655568efd65",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_up = False  # Set to True if you want to delete the resources"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d74595a-473b-4894-8b43-840032bc1e22",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Delete Feature Store resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab384da3-1882-46b3-9b3e-deef73ebf5d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "if clean_up:  \n",
    "    try:\n",
    "        # 1. Delete Feature View\n",
    "        delete_op = admin_client.delete_feature_view(\n",
    "            name=f\"projects/{PROJECT_ID}/locations/{REGION}/featureOnlineStores/{FEATURE_ONLINE_STORE_ID}/featureViews/{FEATURE_VIEW_ID}\"\n",
    "        )\n",
    "        # Wait for Feature View deletion to complete\n",
    "        delete_op.result()\n",
    "        print(\"Feature View deleted successfully!\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error deleting Feature View: {e}\")\n",
    "\n",
    "    try:\n",
    "        # 2. Delete Features \n",
    "        for lro in create_feature_lros:\n",
    "            feature_name = lro.result().name  \n",
    "            registry_client.delete_feature(name=feature_name)\n",
    "        print(\"Features deleted successfully!\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error deleting Features: {e}\")\n",
    "\n",
    "    try:\n",
    "        # 3. Delete Feature Group\n",
    "        registry_client.delete_feature_group(\n",
    "            name=f\"projects/{PROJECT_ID}/locations/{REGION}/featureGroups/{FEATURE_GROUP_ID}\"\n",
    "        )\n",
    "        # Wait for Feature Group deletion to complete\n",
    "        delete_op.result()\n",
    "        print(\"Feature Group deleted successfully!\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error deleting Feature Group: {e}\")\n",
    "\n",
    "    try:\n",
    "        # 4. Delete Feature Online Store\n",
    "        admin_client.delete_feature_online_store(\n",
    "            name=f\"projects/{PROJECT_ID}/locations/{REGION}/featureOnlineStores/{FEATURE_ONLINE_STORE_ID}\"\n",
    "        )\n",
    "        print(\"Feature Online Store deleted successfully!\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error deleting Feature Online Store: {e}\")\n",
    "\n",
    "else:\n",
    "    print(\"clean_up parameter is set to False.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22cf7da1-b4de-494d-9130-b62d53041a18",
   "metadata": {},
   "source": [
    "## Delete BigQuery dataset and view\n",
    "\n",
    "# WARNING: THE BIGQUERY DATASET AND VIEW CREATED IN THIS NOTEBOOK ARE ALSO USED IN CHAPTER 14. IF YOU PLAN TO PROCEED WITH THE ACTIVITIES IN CHAPTER 14, DO NOT DELETE THESE RESOURCES YET. \n",
    "\n",
    "If you want to delete the BigQuery resources, set the delete_bq parameter to True."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f545cf4-03c6-4462-bb86-b99e199059dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "delete_bq = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cbc0c71-2676-4dc1-8e4f-c8262193f882",
   "metadata": {},
   "outputs": [],
   "source": [
    "if delete_bq:  \n",
    "    try:\n",
    "        client.delete_table(view_id, not_found_ok=True)\n",
    "        print(f\"Deleted view: {view_id}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error deleting view: {e}\")\n",
    "\n",
    "    try:\n",
    "        client.delete_dataset(dataset_id, delete_contents=True, not_found_ok=True)\n",
    "        print(f\"Deleted dataset: {dataset_id}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error deleting dataset: {e}\")\n",
    "        \n",
    "else:\n",
    "    print(\"delete_bq parameter is set to False.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc2f8a5-4748-4be8-bc1e-f5007f00d471",
   "metadata": {},
   "source": [
    "## Delete GCS Bucket\n",
    "The bucket can be reused throughout multiple activities in the book. Sometimes, activities in certain chapters make use of artifacts from previous chapters that are stored in the GCS bucket.\n",
    "\n",
    "I highly recommend **not deleting the bucket** unless you will be performing no further activities in the book. For this reason, there's a separate `delete_bucket` variable to specify if you want to delete the bucket.\n",
    "\n",
    "If you want to delete the bucket, set the `delete_bucket` parameter to `True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f2e150-d910-415e-add3-b8274ee766da",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "delete_bucket = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d4ca201-3eaa-4e13-b92a-6b8902df7ef5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if delete_bucket == True:\n",
    "    # Delete the bucket\n",
    "    ! gcloud storage rm --recursive gs://$BUCKET\n",
    "else:\n",
    "    print(\"delete_bucket parameter is set to False\")"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-cpu.2-11.m121",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/tf2-cpu.2-11:m121"
  },
  "kernelspec": {
   "display_name": "Python 3 (Local)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
